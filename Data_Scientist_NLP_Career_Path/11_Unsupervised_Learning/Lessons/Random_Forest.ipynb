{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ramdom forest\n",
    "\n",
    "A random forest is an ensemble machine learning technique. A random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins.Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact.\n",
    "\n",
    "## Bootstrapping\n",
    "\n",
    "Right now, our algorithm for creating a decision tree is deterministic — given a training set, the same tree will be made every time. To make a random forest, we use a technique called bagging, which is short for bootstrap aggregating.\n",
    "\n",
    "How it works is as follows: every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.\n",
    "\n",
    "In bootstrapping, we’re doing this process with replacement. Picture putting all 100 rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag. This means that when we’re picking our 100 random rows, we could pick the same row more than once. In fact, it’s very unlikely, but all 100 randomly picked rows could all be the same row! Because we’re picking these rows with replacement, there’s no need to shrink our bagged training set from 1000 rows to 100. We can pick 1000 rows at random, and because we can get the same row more than once, we’ll still end up with a unique data set.\n",
    "\n",
    "We’ve loaded a dataset about cars here. An important field within the dataset is the safety rating denoted by safety that can be “low”, “med”, or “high.” We’re going to implement bootstrapping and estimate the average safety rating across the different bootstrapped samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of safety ratings in 1728 of data:\n",
      "safety\n",
      "low     0.333333\n",
      "med     0.333333\n",
      "high    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of safety ratings in bootstrapped sample data:\n",
      "safety\n",
      "low     0.344329\n",
      "high    0.333333\n",
      "med     0.322338\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average low percentage: 0.3332\n",
      "95% Confidence Interval for low percengage: (0.3119,0.3553)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "\n",
    "# Below line is transforming the 'accep' columns into a boolean series \n",
    "# where True means the car is acceptable\n",
    "# and False means it is not acceptable\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "\n",
    "# convert categorical variables into dummy variables (numerical ones)\n",
    "# select all rows (:) and the first 6 columns of the dataframe df\n",
    "# Drp the first dummy variable to avoid multicolinearity\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "# Get number of rows. Shape returns a tuple representing database dimensions\n",
    "nrows = df.shape[0]\n",
    "\n",
    "## 1. Print number of rows and distribution of safety ratings \n",
    "print(f'Distribution of safety ratings in {nrows} of data:')\n",
    "print(df.safety.value_counts(normalize=True))\n",
    "\n",
    "## 2. Create bootstrapped sample\n",
    "# Create a sample the same size as the real dataset\n",
    "boot_sample = df.sample(nrows, replace=True)\n",
    "print(f'Distribution of safety ratings in bootstrapped sample data:')\n",
    "print(boot_sample.safety.value_counts(normalize=True))\n",
    "\n",
    "## 3a. Create 1000 bootstrapped samples\n",
    "low_perc = []\n",
    "for i in range(1000):\n",
    "    boot_sample = df.sample(nrows, replace=True)\n",
    "    low_perc.append(boot_sample.safety.value_counts(normalize=True)['low'])\n",
    "\n",
    "## 3b. Plot a histogram of the low percentage values\n",
    "mean_lp = np.mean(low_perc) \n",
    "plt.hist(low_perc, bins=20);\n",
    "plt.xlabel('Low Percentage')\n",
    "plt.show()\n",
    "\n",
    "## 4. What are the 2.5 and 97.5 percentiles?\n",
    "mean_lp = np.mean(low_perc)\n",
    "print(f'Average low percentage: {np.mean(low_perc).round(4)}')\n",
    "\n",
    "low_perc.sort()\n",
    "print(f'95% Confidence Interval for low percengage: ({low_perc[25].round(4)},{low_perc[975].round(4)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging\n",
    "\n",
    "Random forests create different trees using a process known as bagging, which is short for bootstrapped aggregating. As we already covered bootstrapping, the process starts with creating a single decision tree on a bootstrapped sample of data points in the training set. Then after many trees have been made, the results are “aggregated” together. In the case of a classification task, often the aggregation is taking the majority vote of the individual classifiers. For regression tasks, often the aggregation is the average of the individual regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of DT on test set (trained using full set): 0.8588\n",
      "Accuracy score of DT on test set (trained using bootstrapped sample): 0.8912\n",
      "Accuracy score of aggregated 10 bootstrapped samples:0.9097\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "#original decision tree trained on full training set\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(x_train, y_train)\n",
    "print(f'Accuracy score of DT on test set (trained using full set): {dt.score(x_test, y_test).round(4)}')\n",
    "\n",
    "#2. New decision tree trained on bootstrapped sample\n",
    "dt2 = DecisionTreeClassifier(max_depth=5)\n",
    "#ids are the indices of the bootstrapped sample\n",
    "ids = x_train.sample(x_train.shape[0], replace=True, random_state=0).index\n",
    "dt2.fit(x_train.loc[ids], y_train[ids])#max_depth=50,criterion='gini')\n",
    "print(f'Accuracy score of DT on test set (trained using bootstrapped sample): {dt2.score(x_test, y_test).round(4)}')\n",
    "\n",
    "## 3. Bootstapping ten samples and aggregating the results:\n",
    "preds = []\n",
    "random_state = 0\n",
    "for i in range(10):\n",
    "    ids = x_train.sample(x_train.shape[0], replace=True, random_state=random_state+i).index\n",
    "    dt2.fit(x_train.loc[ids], y_train[ids])\n",
    "    preds.append(dt2.predict(x_test))   \n",
    "ba_pred = np.array(preds).mean(0)\n",
    "\n",
    "# 4. Calculate accuracy of the bagged sample\n",
    "ba_accuracy = accuracy_score(ba_pred>=0.5, y_test)\n",
    "print(f'Accuracy score of aggregated 10 bootstrapped samples:{ba_accuracy.round(4)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random feature selection\n",
    "\n",
    "In addition to using bootstrapped samples of our dataset, we can continue to add variety to the ways our trees are created by randomly selecting the features that are used.\n",
    "\n",
    "Recall that for our car data set, the original features were the following:\n",
    "\n",
    "- The price of the car which can be “vhigh”, “high”, “med”, or “low”.\n",
    "- The cost of maintaining the car which can be “vhigh”, “high”, “med”, or “low”.\n",
    "- The number of doors which can be “2”, “3”, “4”, “5more”.\n",
    "- The number of people the car can hold which can be “2”, “4”, or “more”.\n",
    "- The size of the trunk which can be “small”, “med”, or “big”.\n",
    "- The safety rating of the car which can be “low”, “med”, or “high”\n",
    "- Our target variable for prediction is an acceptability rating, accep, that’s either True or False. For our final features sets, x_train and x_test, the categorical features have been dummy encoded, giving us 15 features in total.\n",
    "\n",
    "When we use a decision tree, all the features are used and the split is chosen as the one that increases the information gain the most. While it may seem counter-intuitive, selecting a random subset of features can help in the performance of an ensemble model. In the following example, we will use a random selection of features prior to model building to add additional variance to the individual trees. While an individual tree may perform worse, sometimes the increases in variance can help model performance of the ensemble model as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of DT on test set (trained using full feature set):\n",
      "0.9444444444444444\n",
      "Accuracy score of DT on test set (trained using random feature sample):\n",
      "0.7986111111111112\n",
      "Accuracy score of aggregated 10 samples:\n",
      "0.7638888888888888\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "print(\"Accuracy score of DT on test set (trained using full feature set):\")\n",
    "accuracy_dt = dt.score(x_test, y_test)\n",
    "print(accuracy_dt)\n",
    "\n",
    "# 1. Create rand_features, random samples from the set of features\n",
    "rand_features = np.random.choice(x_train.columns,10)\n",
    "\n",
    "# Make new decision tree trained on random sample of 10 features and calculate the new accuracy score\n",
    "dt2 = DecisionTreeClassifier()\n",
    "\n",
    "dt2.fit(x_train[rand_features], y_train)\n",
    "print(\"Accuracy score of DT on test set (trained using random feature sample):\")\n",
    "accuracy_dt2 = dt2.score(x_test[rand_features], y_test)\n",
    "print(accuracy_dt2)\n",
    "\n",
    "# 2. Build decision trees on 10 different random samples \n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    rand_features = np.random.choice(x_train.columns,10)\n",
    "    dt2.fit(x_train[rand_features], y_train)\n",
    "    predictions.append(dt2.predict(x_test[rand_features]))\n",
    "\n",
    "## 3. Get aggregate predictions and accuracy score\n",
    "prob_predictions = np.array(predictions).mean(0)\n",
    "agg_predictions = (prob_predictions>0.5)\n",
    "agg_accuracy = accuracy_score(agg_predictions, y_test)\n",
    "print('Accuracy score of aggregated 10 samples:')\n",
    "print(agg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit implementation\n",
    "\n",
    "Much like other models we have used in scikit-learn, we instantiate a instance of BaggingClassifier() and specify the parameters. The first parameter, base_estimator refers to the machine learning model that is being bagged. In the case of random forests, the base estimator would be a decision tree. We are going to use a decision tree classifier WITH a max_depth of 5, this will be instantiated with BaggingClassifier(DecisionTreeClassifier(max_depth=5)).\n",
    "\n",
    "After the model has been defined, methods .fit(), .predict(), .score() can be used as expected. Additional hyperparameters specific to bagging include the number of estimators (n_estimators) we want to use and the maximum number of features we’d like to keep (max_features).\n",
    "\n",
    "Note: While we have focused on decision tress classifiers (as this is the base learner for a random forest classifier), this procedure of bagging is not specific to decision trees, and in fact can be used for any base classifier or regression model. The scikit-learn implementation is generalizable and can be used for other base models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged Classifier, 10 estimators:\n",
      "0.9212962962962963\n",
      "Accuracy score of Bagged Classifier, 10 estimators, 10 max features:\n",
      "0.8726851851851852\n",
      "Accuracy score of Logistic Regression, 10 estimators:\n",
      "0.9143518518518519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alan_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alan_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alan_\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Bagging classifier with 10 Decision Tree base estimators\n",
    "bag_dt = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)\n",
    "bag_dt.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators:')\n",
    "bag_accuracy = bag_dt.score(x_test, y_test)\n",
    "print(bag_accuracy)\n",
    "\n",
    "# 2.Set `max_features` to 10.\n",
    "bag_dt_10 = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10)\n",
    "bag_dt_10.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators, 10 max features:')\n",
    "bag_accuracy_10 = bag_dt_10.score(x_test, y_test)\n",
    "print(bag_accuracy_10)\n",
    "\n",
    "\n",
    "# 3. Change base estimator to Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "bag_lr = BaggingClassifier(base_estimator=LogisticRegression(),\n",
    "                         n_estimators=10, max_features=10)\n",
    "bag_lr.fit(x_train, y_train)\n",
    "\n",
    "print('Accuracy score of Logistic Regression, 10 estimators:')\n",
    "bag_accuracy_lr = bag_lr.score(x_test, y_test)\n",
    "print(bag_accuracy_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "Test set accuracy: 0.9490740740740741\n",
      "Test set precision: 0.9523809523809523\n",
      "Test set recall: 0.8823529411764706\n",
      "Test set confusion matrix:\n",
      "[[290   6]\n",
      " [ 16 120]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Forest Classifier and print its parameters\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "print('Random Forest parameters:')\n",
    "rf_params = rf.get_params()\n",
    "print(rf_params)\n",
    "\n",
    "# 2. Fit the Random Forest Classifier to training data and calculate accuracy score on the test data\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "rf_accuracy = rf.score(x_test, y_test)\n",
    "\n",
    "print('Test set accuracy:', rf_accuracy)\n",
    "\n",
    "# 3. Calculate Precision and Recall scores and the Confusion Matrix\n",
    "\n",
    "rf_precision = precision_score(y_test, y_pred)\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "rf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Test set precision: {rf_precision}')\n",
    "print(f'Test set recall: {rf_recall}')\n",
    "print(f'Test set confusion matrix:\\n{rf_confusion_matrix}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor\n",
    "\n",
    "Just like in decision trees, we can use random forests for regression as well! It is important to know when to use regression or classification — this usually comes down to what type of variable your target is. Previously, we were using a binary categorical variable (acceptable versus not), so a classification model was used.\n",
    "\n",
    "We will now consider a hypothetical new target variable, price, for this data set, which is a continuous variable. We’ve generated some fake prices in the dataset so that we have numerical values instead of the previous categorical variables. (Please note that these are not reflective of the previous categories of high and low prices - we just wanted some numeric values so we can perform regression! :) )\n",
    "\n",
    "Now, instead of a classification task, we will use scikit-learn‘s RandomForestRegressor() to carry out a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "\n",
    "## Generating some fake prices for regression! :) \n",
    "fake_prices = (15000 + 25*df.index.values)+np.random.normal(size=df.shape[0])*5000\n",
    "df['price'] = fake_prices\n",
    "print(df.price.describe())\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Regressor and print `R^2` scores on training and test data\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "r_squared_train = rfr.score(x_train, y_train)\n",
    "print(f'Train set R^2: {r_squared_train}')\n",
    "\n",
    "r_squared_test = rfr.score(x_test, y_test)\n",
    "print(f'Test set R^2: {r_squared_test}')\n",
    "\n",
    "# 2. Print Mean Absolute Error on training and test data\n",
    "\n",
    "avg_price = y.mean()\n",
    "print(f'Avg Price Train/Test: {avg_price}')\n",
    "\n",
    "y_pred_train =rfr.predict(x_train)\n",
    "y_pred_test =rfr.predict(x_test)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "print(f'Train set MAE: {mae_train}')\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "print(f'Test set MAE: {mae_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
