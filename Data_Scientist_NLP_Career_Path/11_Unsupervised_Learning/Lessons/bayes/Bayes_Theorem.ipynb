{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes theorem\n",
    "\n",
    "Two events are independent if the occurrence of one event does not affect the probability of the second event\n",
    "\n",
    "Bayes' Theorem deals with probability of dependent events. It takes a test result and relates it to the conditional probability of that test result given other related events. \n",
    "\n",
    "we determined two probabilities:\n",
    "\n",
    "1. The patient had the disease, and the test correctly diagnosed the disease ≈ 0.00001\n",
    "2. The patient didn’t have the disease and the test incorrectly diagnosed that they had the disease ≈ 0.01\n",
    "\n",
    "Both events are rare, but we can see that it was about 1,000 times more likely that the test was incorrect than that the patient had this rare disease.\n",
    "\n",
    "We’re able to come to this conclusion because we had more information than just the accuracy of the test; we also knew the prevalence of this disease.\n",
    "\n",
    "In statistics, if we have two events (A and B), we write the probability that event A will happen, given that event B already happened as P(A|B). In our example, we want to find P(rare disease | positive result). In other words, we want to find the probability that the patient has the disease given the test came back positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9899999999999999\n",
      "1e-05\n",
      "0.01001\n",
      "0.000989010989010989\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p_positive_given_disease = (0.99 * (.00001))/ (1./100000.)\n",
    "print(p_positive_given_disease)\n",
    "\n",
    "p_disease = 1./100000.\n",
    "print(p_disease)\n",
    "\n",
    "p_positive = (0.00001) + (0.01) \n",
    "print(p_positive)\n",
    "\n",
    "p_disease_given_positive = (p_positive_given_disease) * (p_disease) / (p_positive)\n",
    "\n",
    "print(p_disease_given_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filters\n",
    "\n",
    "Let’s explore a different example. Email spam filters use Bayes’ Theorem to determine if certain words indicate that an email is spam.\n",
    "\n",
    "Let’s take a word that often appears in spam: “enhancement”.\n",
    "\n",
    "With just 3 facts, we can make some preliminary steps towards a good spam filter:\n",
    "\n",
    "1. “enhancement” appears in just 0.1% of non-spam emails\n",
    "2. “enhancement” appears in 5% of spam emails\n",
    "3. Spam emails make up about 20% of total emails\n",
    "\n",
    "Given that an email contains “enhancement”, what is the probability that the email is spam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9259259259259259\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 'spam'\n",
    "b = 'enhancement'\n",
    "\n",
    "p_spam = 0.2\n",
    "p_enhancement_given_spam = 0.05\n",
    "p_enhancement = 0.05 * 0.2 + 0.001 * (1 - 0.2)\n",
    "p_spam_enhancement = p_enhancement_given_spam * p_spam / p_enhancement\n",
    "\n",
    "print(p_spam_enhancement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Naive Bayes Classifier\n",
    "\n",
    "A Naive Bayes classifier is a supervised machine learning algorithm that leverages Bayes’ Theorem to make predictions and classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from reviews import neg_list, pos_list\\nfrom sklearn.feature_extraction.text import CountVectorizer\\n\\nreview = \"This crib was amazing\"\\n\\n# Create a count vectorizer\\ncounter = CountVectorizer()\\n\\n# Learn vocabilary from both sets of strings\\ncounter.fit(neg_list + pos_list)\\n\\n# This is the vocabulary that your counter just learned. \\n# The numbers associated with each word are the indices of each word when you transform a review.\\nprint(counter.vocabulary_)\\n\\n# Transform our review\\nreview_counts = counter.transform([review])\\nprint(review_counts.toarray())\\n\\n# Transform training set\\ntraining_counts = counter.transform(neg_list + pos_list)\\n\\nreview = \"This crib was great amazing and wonderful\"\\nreview_counts = counter.transform([review])\\n\\n# Create classifier\\nclassifier = MultinomialNB()\\n\\n# Create labels\\n# We made the training points by combining neg_list and pos_list. \\n# So the first half of the labels should be 0 (for negative) and the second half should be 1 (for positive).\\n# Create a list named training_labels that has 1000 0s followed by 1000 1s.\\ntraining_labels = [0] * 1000 + [1] * 1000\\n\\n# Fit using training set and labels\\nclassifier.fit(training_counts, training_labels)\\nprint(classifier.predict(review_counts))\\nprint(classifier.predict_proba(review_counts))'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from reviews import neg_list, pos_list\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "review = \"This crib was amazing\"\n",
    "\n",
    "# Create a count vectorizer\n",
    "counter = CountVectorizer()\n",
    "\n",
    "# Learn vocabilary from both sets of strings\n",
    "counter.fit(neg_list + pos_list)\n",
    "\n",
    "# This is the vocabulary that your counter just learned. \n",
    "# The numbers associated with each word are the indices of each word when you transform a review.\n",
    "print(counter.vocabulary_)\n",
    "\n",
    "# Transform our review\n",
    "review_counts = counter.transform([review])\n",
    "print(review_counts.toarray())\n",
    "\n",
    "# Transform training set\n",
    "training_counts = counter.transform(neg_list + pos_list)\n",
    "\n",
    "review = \"This crib was great amazing and wonderful\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "# Create classifier\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Create labels\n",
    "# We made the training points by combining neg_list and pos_list. \n",
    "# So the first half of the labels should be 0 (for negative) and the second half should be 1 (for positive).\n",
    "# Create a list named training_labels that has 1000 0s followed by 1000 1s.\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "# Fit using training set and labels\n",
    "classifier.fit(training_counts, training_labels)\n",
    "print(classifier.predict(review_counts))\n",
    "print(classifier.predict_proba(review_counts))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, you’ve learned how to leverage Bayes’ Theorem to create a supervised machine learning algorithm. Here are some of the major takeaways from the lesson:\n",
    "\n",
    "- A tagged dataset is necessary to calculate the probabilities used in Bayes’ Theorem.\n",
    "- In this example, the features of our dataset are the words used in a product review. In order to apply Bayes’ Theorem, we assume that these features are independent.\n",
    "- Using Bayes’ Theorem, we can find P(class|data point) for every possible class. In this example, there were two classes — positive and negative. The class with the highest probability will be the algorithm’s prediction.\n",
    "\n",
    "Even though our algorithm is running smoothly, there’s always more that we can add to try to improve performance. The following techniques are focused on ways in which we process data before feeding it into the Naive Bayes classifier:\n",
    "\n",
    "- Remove punctuation from the training set. Right now in our dataset, there are 702 instances of \"great!\" and 2322 instances of \"great.\". We should probably combine those into 3024 instances of \"great\".\n",
    "- Lowercase every word in the training set. We do this for the same reason why we remove punctuation. We want \"Great\" and \"great\" to be the same.\n",
    "- Use a bigram or trigram model. Right now, the features of a review are individual words. For example, the features of the point “This crib is great” are “This”, “crib”, “is”, and “great”. If we used a bigram model, the features would be “This crib”, “crib is”, and “is great”. Using a bigram model makes the assumption of independence more reasonable.\n",
    "\n",
    "Smoothing in a Naive Bayes Classifier is done to prevent a feature with a porbability of 0 from ruining the total probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes project using scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.sport.baseball', 'rec.sport.hockey']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "emails = fetch_20newsgroups(categories = ['rec.sport.baseball', 'rec.sport.hockey'])\n",
    "\n",
    "print(emails.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: mmb@lamar.ColoState.EDU (Michael Burger)\n",
      "Subject: More TV Info\n",
      "Distribution: na\n",
      "Nntp-Posting-Host: lamar.acns.colostate.edu\n",
      "Organization: Colorado State University, Fort Collins, CO  80523\n",
      "Lines: 36\n",
      "\n",
      "United States Coverage:\n",
      "Sunday April 18\n",
      "  N.J./N.Y.I. at Pittsburgh - 1:00 EDT to Eastern Time Zone\n",
      "  ABC - Gary Thorne and Bill Clement\n",
      "\n",
      "  St. Louis at Chicago - 12:00 CDT and 11:00 MDT - to Central/Mountain Zones\n",
      "  ABC - Mike Emerick and Jim Schoenfeld\n",
      "\n",
      "  Los Angeles at Calgary - 12:00 PDT and 11:00 ADT - to Pacific/Alaskan Zones\n",
      "  ABC - Al Michaels and John Davidson\n",
      "\n",
      "Tuesday, April 20\n",
      "  N.J./N.Y.I. at Pittsburgh - 7:30 EDT Nationwide\n",
      "  ESPN - Gary Thorne and Bill Clement\n",
      "\n",
      "Thursday, April 22 and Saturday April 24\n",
      "  To Be Announced - 7:30 EDT Nationwide\n",
      "  ESPN - To Be Announced\n",
      "\n",
      "\n",
      "Canadian Coverage:\n",
      "\n",
      "Sunday, April 18\n",
      "  Buffalo at Boston - 7:30 EDT Nationwide\n",
      "  TSN - ???\n",
      "\n",
      "Tuesday, April 20\n",
      "  N.J.D./N.Y. at Pittsburgh - 7:30 EDT Nationwide\n",
      "  TSN - ???\n",
      "\n",
      "Wednesday, April 21\n",
      "  St. Louis at Chicago - 8:30 EDT Nationwide\n",
      "  TSN - ???\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(emails.data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(emails.target[5])\n",
    "\n",
    "# Label value is 1 so this email is about Hockey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware','rec.sport.hockey'], subset = 'train', shuffle = True, random_state = 108)\n",
    "\n",
    "test_emails = fetch_20newsgroups(categories = ['comp.sys.ibm.pc.hardware','rec.sport.hockey'], subset = 'test', shuffle = True, random_state = 108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974715549936789\n"
     ]
    }
   ],
   "source": [
    "# Transform these emails into a list of word counts\n",
    "counter = CountVectorizer()\n",
    "\n",
    "#  tell counter what possible words can exist in our emails\n",
    "counter.fit(test_emails.data + train_emails.data)\n",
    "\n",
    "# make a list of the counts of our words in our training set.\n",
    "train_counts = counter.transform(train_emails.data)\n",
    "\n",
    "# make a list of the counts of our words in our test set.\n",
    "test_counts = counter.transform(test_emails.data)\n",
    "\n",
    "# Create an instance of a Naive Bayes classifier that we can train and test on\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "# Call classifier‘s .fit() function. .fit() takes two parameters. \n",
    "# The first should be our training set, which for us is train_counts. \n",
    "# The second should be the labels associated with the training emails.\n",
    "classifier.fit(train_counts, train_emails.target)\n",
    "\n",
    "# Test the Naive Bayes Classifier by printing classifier‘s .score() function. \n",
    "# .score() takes the test set and the test labels as parameters.\n",
    "print(classifier.score(test_counts, test_emails.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
