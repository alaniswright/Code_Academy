{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with deep learning\n",
    "\n",
    "# Note that none of this works on my PC because my graphics card isn't good enough to run TensorFlow!\n",
    "\n",
    "\n",
    "**When I have a computer than can run tensorFlow, redo this using the TensorFlow tutorial https://keras.io/examples/nlp/lstm_seq2seq/**\n",
    "\n",
    "Documentation: Keras: Natural Language Processing https://keras.io/examples/nlp/\n",
    "\n",
    "Tutorial: TensorFlow: Neural Machine Translation with Attention https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
    "\n",
    "One of the most common neural models used for text generation is the sequence-to-sequence model, commonly referred to as seq2seq (pronounced “seek-to-seek”). A type of encoder-decoder model, seq2seq uses recurrent neural networks (RNNs) like LSTM in order to generate output, token by token or character by character. Examples of seq2seq:\n",
    "\n",
    "- Machine translation software like Google Translate\n",
    "- Text summary generation\n",
    "- Chatbots\n",
    "- Named Entity Recognition (NER)\n",
    "- Speech recognition\n",
    "\n",
    "Seq2seq networks have two parts:\n",
    "\n",
    "- An encoder that accepts language (or audio or video) input. The output matrix of the encoder is discarded, but its state is preserved as a vector.\n",
    "- A decoder that takes the encoder’s final state (or memory) as its initial state. We use a technique called “teacher forcing” to train the decoder to predict the following text (characters or words) in a target sequence given the previous text.\n",
    "\n",
    "\n",
    "## Preprocessing for seq2sec\n",
    "\n",
    "We’ll be using TensorFlow with the Keras API to build a pretty limited English-to-Spanish translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alan_\\Documents\\GitHub\\Code-Academy\\Python_For_Data_Science\\13-NaturalLanguageProcessing\\Lessons\\Text_Generation.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/Text_Generation.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/Text_Generation.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/Text_Generation.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Importing our translations\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import re\n",
    "# Importing our translations\n",
    "data_path = \"span-eng.txt\"\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "\n",
    "# Building empty lists to hold sentences\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "# Building empty vocabulary sets\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "for line in lines:\n",
    "  # Input and target sentences are separated by tabs\n",
    "  input_doc, target_doc = line.split('\\t')\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_docs.append(input_doc)\n",
    "  # Splitting words from punctuation\n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below \n",
    "  # and append it to target_docs:\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)\n",
    "  \n",
    "  # Now we split up each sentence into words\n",
    "  # and add each unique word to our vocabulary set\n",
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    print(token)\n",
    "    # Add your code here:\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    print(token)\n",
    "    # And here:\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "# Create num_encoder_tokens and num_decoder_tokens:\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "try:\n",
    "  max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "  max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "except ValueError:\n",
    "  pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
