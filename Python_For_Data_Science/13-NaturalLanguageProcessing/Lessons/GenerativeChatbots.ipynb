{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common neural models used for text generation is the sequence-to-sequence model, commonly referred to as seq2seq (pronounced “seek-to-seek”). A type of encoder-decoder model, seq2seq uses recurrent neural networks (RNNs) like LSTM in order to generate output, token by token or character by character.\n",
    "\n",
    "seq2seq networks have two parts:\n",
    "\n",
    "- An encoder that accepts language (or audio or video) input. The output matrix of the encoder is discarded, but its state is preserved as a vector.\n",
    "- A decoder that takes the encoder’s final state (or memory) as its initial state. We use a technique called “teacher forcing” to train the decoder to predict the following text (characters or words) in a target sequence given the previous text.\n",
    "\n",
    "While closed-domain architecture is focused on response selection from a set of predefined responses, generative architecture allows us to perform unbounded text generation. Instead of selecting full sentences, the open-domain model generates word by word or character by character, allowing for new combinations of language.\n",
    "\n",
    "In the seq2seq decoding function, the decoder generates several possible output tokens and the one with the highest probability (according to the model) gets selected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'more_itertools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alan_\\Documents\\GitHub\\Code-Academy\\Python_For_Data_Science\\13-NaturalLanguageProcessing\\Lessons\\GenerativeChatbots.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/GenerativeChatbots.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmore_itertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmit\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'more_itertools'"
     ]
    }
   ],
   "source": [
    "\n",
    "import more_itertools as mit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
