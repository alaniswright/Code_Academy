{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "\n",
    "This idea that a word’s meaning can be understood by its context, or the words that surround it, is the basis for word embeddings. A word embedding is a representation of a word as a numeric vector, enabling us to compare and contrast how words are used and identify words that occur in similar contexts.\n",
    "\n",
    "Requires words to be represented as vectors. Vectors are represented as arrays.\n",
    "\n",
    "Arrays are for elements of the same data type. They are used specifically for lists which need to have mathematical operations done on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores_xavier = np.array([88, 92])\n",
    "\n",
    "scores_niko = np.array([94, 87])\n",
    "\n",
    "scores_alena = np.array([90, 48])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are vector representations of a word.\n",
    "\n",
    "They allow us to take all the information that is stored in a word, like its meaning and its part of speech, and convert it into a numeric form that is more understandable to a computer.\n",
    "\n",
    "We can load a basic English word embedding model using spaCy as follows:\n",
    "\n",
    "To get the vector representation of a word, we call the model with the desired word as an argument and can use the .vector attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "love_vector = nlp('love').vector\n",
    "\n",
    "print(len(love_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance\n",
    "\n",
    "The key at the heart of word embeddings is distance.\n",
    "\n",
    "Manhattan distance:  also known as city block distance, distance is defined as the sum of the differences across each individual dimension of the vectors\"\n",
    "\n",
    "Euclidean distance, also known as straight line distance. With this distance metric, we take the square root of the sum of the squares of the differences in each dimension.\n",
    "\n",
    "Cosine distance is concerned with the angle between two vectors, rather than by looking at the distance between the points, or ends, of the vectors. Two vectors that point in the same direction have no angle between them, and have a cosine distance of 0. Two vectors that point in opposite directions, on the other hand, have a cosine distance of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3.7416573867739413\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
    "\n",
    "vector_a = np.array([1,2,3])\n",
    "vector_b = np.array([2,4,6])\n",
    "\n",
    "# Manhattan distance:\n",
    "manhattan_d = cityblock(vector_a,vector_b) # 6\n",
    "print(manhattan_d)\n",
    "\n",
    "# Euclidean distance:\n",
    "euclidean_d = euclidean(vector_a,vector_b) # 3.74\n",
    "print(euclidean_d)\n",
    "\n",
    "# Cosine distance:\n",
    "cosine_d = cosine(vector_a,vector_b) # 0.0\n",
    "print(cosine_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind word embeddings is a theory known as the distributional hypothesis. This hypothesis states that words that co-occur in the same contexts tend to have similar meanings. With word embeddings, we map words that exist with the same context to similar places in our vector space (math-speak for the area in which our vectors exist).\n",
    "\n",
    "The numeric values that are assigned to the vector representation of a word are not important in their own right, but gather meaning from how similar or not words are to each other.\n",
    "\n",
    "The literal values of a word’s embedding have no actual meaning. We gain value in word embeddings from comparing the different word vectors and seeing how similar or different they are. Encoded in these vectors, however, is latent information about how they are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to vec\n",
    "\n",
    "Word2vec is a statistical learning algorithm that develops word embeddings from a corpus of text. Word2vec uses one of two different model architectures to come up with the values that define a collection of word embeddings.\n",
    "\n",
    "One method is to use the continuous bag-of-words (CBOW) representation of a piece of text. The word2vec model goes through each word in the training corpus, in order, and tries to predict what word comes at each position based on applying bag-of-words to the words that surround the word in question. In this approach, the order of the words does not matter!\n",
    "\n",
    "The other method word2vec can use to create word embeddings is continuous skip-grams. Skip-grams function similarly to n-grams, except instead of looking at groupings of n-consecutive words in a text, we can look at sequences of words that are separated by some specified distance between them.\n",
    "\n",
    "When using continuous skip-grams, the order of context is taken into consideration! Because of this, the time it takes to train the word embeddings is slower than when using continuous bag-of-words. The results, however, are often much better!\n",
    "\n",
    "With either the continuous bag-of-words or continuous skip-grams representations as training data, word2vec then uses a shallow, 2-layer neural network to come up with the values that place words with a similar context in vectors near each other and words with different contexts in vectors far apart from each other.\n",
    "\n",
    "## Gensim\n",
    "\n",
    "Spacy is trained by the Linguistic Data Consortium.\n",
    "\n",
    "Gensim allow us to train our own word embeddings model on our own corpus of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alan_\\Documents\\GitHub\\Code-Academy\\Python_For_Data_Science\\13-NaturalLanguageProcessing\\Lessons\\Word_Embeddings.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/Word_Embeddings.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alan_/Documents/GitHub/Code-Academy/Python_For_Data_Science/13-NaturalLanguageProcessing/Lessons/Word_Embeddings.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mWord2Vec(corpus, size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, sg\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus, size=100, window=5, min_count=1, workers=2, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- corpus is a list of lists, where each inner list is a document in the corpus and each element in the inner lists is a word token\n",
    "- size determines how many dimensions our word embeddings will include. Word embeddings often have upwards of 1,000 dimensions! Here we will create vectors of 100-dimensions to keep things simple.\n",
    "- don’t worry about the rest of the keyword arguments here!\n",
    "\n",
    "To view the entire vocabulary used to train the word embedding model, we can use the .wv.vocab.items() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_of_model = list(model.wv.vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train a word2vec model on a smaller corpus of text, we pick up on the unique ways in which words of the text are used.\n",
    "\n",
    "For example, if we were using scripts from the television show Friends as a training corpus, the model would pick up on the unique ways in which words are used in the show. While the generalized vectors in a spaCy model might not place the vectors for “Ross” and “Rachel” close together, a gensim word embedding model trained on Friends’ scripts would place the vectors for words like “Ross” and “Rachel”, two characters that have a continuous on and off-again relationship throughout the show, very close together!\n",
    "\n",
    "To easily find which vectors gensim placed close together in its word embedding model, we can use the .most_similar() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(\"my_word_here\", topn=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"my_word_here\" is the target word token we want to find most similar words to\n",
    "- topn is a keyword argument that indicates how many similar word vectors we want returned\n",
    "\n",
    "One last gensim method we will explore is a rather fun one: .doesnt_match()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match([\"asia\", \"mars\", \"pluto\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when given a list of terms in the vocabulary as an argument, .doesnt_match() returns which term is furthest from the others.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
