{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import spacy\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# None of this is neeeded as the inaugural library does it all already\n",
    "\n",
    "def read_file(file_name):\n",
    "  with open(file_name, 'r+', encoding='utf-8') as file:\n",
    "    file_text = file.read()\n",
    "  return file_text\n",
    "\n",
    "def process_speeches(speeches):\n",
    "  word_tokenized_speeches = list()\n",
    "  for speech in speeches:\n",
    "    sentence_tokenizer = PunktSentenceTokenizer()\n",
    "    sentence_tokenized_speech = sentence_tokenizer.tokenize(speech)\n",
    "    word_tokenized_sentences = list()\n",
    "    for sentence in sentence_tokenized_speech:\n",
    "      word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
    "      word_tokenized_sentences.append(word_tokenized_sentence)\n",
    "    word_tokenized_speeches.append(word_tokenized_sentences)\n",
    "  return word_tokenized_speeches\n",
    "\n",
    "def merge_speeches(speeches):\n",
    "  all_sentences = list()\n",
    "  for speech in speeches:\n",
    "    for sentence in speech:\n",
    "      all_sentences.append(sentence)\n",
    "  return all_sentences\n",
    "\n",
    "def get_president_sentences(president):\n",
    "  files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
    "  speeches = [read_file(file) for file in files]\n",
    "  processed_speeches = process_speeches(speeches)\n",
    "  all_sentences = merge_speeches(processed_speeches)\n",
    "  return all_sentences\n",
    "\n",
    "def get_presidents_sentences(presidents):\n",
    "  all_sentences = list()\n",
    "  for president in presidents:\n",
    "    files = sorted([file for file in os.listdir() if president.lower() in file.lower()])\n",
    "    speeches = [read_file(file) for file in files]\n",
    "    processed_speeches = process_speeches(speeches)\n",
    "    all_prez_sentences = merge_speeches(processed_speeches)\n",
    "    all_sentences.extend(all_prez_sentences)\n",
    "  return all_sentences\n",
    "\n",
    "def most_frequent_words(list_of_sentences):\n",
    "  all_words = [word for sentence in list_of_sentences for word in sentence]\n",
    "  return Counter(all_words).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create word embeddings on the corpus of all the presidents’ speeches, we need to read the text data from each file, separate the files into sentences on a word by word basis, and then merge all the sentences across the speeches into one big list of lists.\n",
    "\n",
    "Let’s start by finding all the file names for the .txt files we will be analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([file for file in os.listdir() if file[-4:] == '.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fellow\n",
      "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':']\n",
      "[['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', 'and', 'of', 'the', 'House', 'of', 'Representatives', ':'], ['Among', 'the', 'vicissitudes', 'incident', 'to', 'life', 'no', 'event', 'could', 'have', 'filled', 'me', 'with', 'greater', 'anxieties', 'than', 'that', 'of', 'which', 'the', 'notification', 'was', 'transmitted', 'by', 'your', 'order', ',', 'and', 'received', 'on', 'the', '14th', 'day', 'of', 'the', 'present', 'month', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "files = inaugural.fileids()\n",
    "\n",
    "speeches = []\n",
    "\n",
    "for file in files:\n",
    "    line = inaugural.sents(file)\n",
    "    speeches.append(line)\n",
    "\n",
    "print(speeches[0][0][0])\n",
    "print(speeches[0][0])\n",
    "print(speeches[0])\n",
    "# processed_speeches = process_speeches(speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of sentences for all presidents\n",
    "sentences_all_presidents = [' '.join(speech) for speech in inaugural.sents()]\n",
    "\n",
    "# Function to tokenize all sentences and remove punctuation, put in consistent case, etc\n",
    "def tokenize_sentences(sentences_to_tokenize):\n",
    "    word_tokenized_sentences = []\n",
    "    for sentence in sentences_to_tokenize:\n",
    "        for word in sentence:\n",
    "            word_tokenized_sentence = [word.lower().strip('.').strip('?').strip('!') for word in sentence.replace(\",\",\"\").replace(\"-\",\" \").replace(\":\",\"\").split()]\n",
    "            word_tokenized_sentences.append(word_tokenized_sentence)\n",
    "    return word_tokenized_sentences\n",
    "\n",
    "# Tokenize sentences from all presidents\n",
    "tokenized_sentences_all_presidents = tokenize_sentences(sentences_all_presidents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most frequent words\n",
    "def most_freq_words(tokenized_sentences):\n",
    "    sent_in_speeches = [word for sentence in tokenized_sentences for word in sentence]\n",
    "    return Counter(sent_in_speeches).most_common()\n",
    "\n",
    "# Find most frequent words for all presidents\n",
    "most_freq_words_all_presidents = most_freq_words(tokenized_sentences_all_presidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inextricably', 0.5242308974266052), ('a', 0.4748968780040741), ('baggage', 0.45908448100090027), ('deathless', 0.45783382654190063), ('the', 0.45011886954307556), ('speech', 0.44889965653419495), ('in', 0.44363024830818176), ('reborn', 0.43920230865478516), ('defects', 0.4300992488861084), ('values', 0.42966604232788086)]\n"
     ]
    }
   ],
   "source": [
    "# Create a word embedding model with gensim\n",
    "# Train it on the sentences in speeches\n",
    "# Iniitally was vector_size=96, window=5, min_count=1, workers=2, sg=1. Reduced to get it to go faster\n",
    "all_prez_embeddings = gensim.models.Word2Vec(tokenized_sentences_all_presidents, vector_size=96, window=2, min_count=10, workers=2, sg=1)\n",
    "\n",
    "# Find words most similar to 'freedom'\n",
    "similar_to_freedom = all_prez_embeddings.wv.most_similar('freedom')\n",
    "print(similar_to_freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('story', 0.9695708155632019), ('policy', 0.9658669233322144), ('land', 0.9640661478042603), ('unity', 0.964009165763855), ('condition', 0.9630237817764282), ('very', 0.9629682898521423), ('basis', 0.9629378914833069), ('party', 0.9629285931587219), ('feeling', 0.9620798826217651), ('sound', 0.9620466232299805)]\n"
     ]
    }
   ],
   "source": [
    "# Find words most similar to 'spirit'\n",
    "similar_to_spirit = all_prez_embeddings.wv.most_similar('spirit')\n",
    "print(similar_to_spirit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fun aspect of word embeddings is to see how different corpora result in different word embeddings, alluding to differences in how words are used between writers/authors/speakers.\n",
    "\n",
    "Let’s train a word embedding model on a single president and see how their word embeddings differ from the collection of all presidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('privilege', 0.7726589441299438), ('ebbing', 0.6769092082977295), ('tide', 0.6420332789421082), ('voices', 0.59122234582901), ('wave', 0.5841450691223145), ('rises', 0.551307737827301), ('story', 0.542332649230957), ('should', 0.539277195930481), ('surging', 0.5379772782325745), ('heed', 0.5236226916313171)]\n"
     ]
    }
   ],
   "source": [
    "# sentences_roosevelt = inaugural.sents(['1945-Roosevelt.txt', '1941-Roosevelt.txt', '1937-Roosevelt.txt', '1933-Roosevelt.txt'])\n",
    "\n",
    "sentences_roosevelt = [' '.join(sublist) for sublist in inaugural.sents(['1945-Roosevelt.txt', '1941-Roosevelt.txt', '1937-Roosevelt.txt', '1933-Roosevelt.txt'])]\n",
    "\n",
    "tokenized_sentences_roosevelt = tokenize_sentences(sentences_roosevelt)\n",
    "\n",
    "most_freq_words_roosevelt = most_freq_words(tokenized_sentences_roosevelt)\n",
    "\n",
    "roosevelt_embeddings = gensim.models.Word2Vec(tokenized_sentences_roosevelt, vector_size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Find words most similar to 'freedom'\n",
    "similar_to_freedom = roosevelt_embeddings.wv.most_similar('freedom')\n",
    "print(similar_to_freedom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_roosevelt = inaugural.sents(['1945-Roosevelt.txt', '1941-Roosevelt.txt', '1937-Roosevelt.txt', '1933-Roosevelt.txt'])\n",
    "\n",
    "sentences_roosevelt = [' '.join(sublist) for sublist in inaugural.sents(['1945-Roosevelt.txt', '1941-Roosevelt.txt', '1937-Roosevelt.txt', '1933-Roosevelt.txt'])]\n",
    "\n",
    "tokenized_sentences_roosevelt = tokenize_sentences(sentences_roosevelt)\n",
    "\n",
    "most_freq_words_roosevelt = most_freq_words(tokenized_sentences_roosevelt)\n",
    "\n",
    "roosevelt_embeddings = gensim.models.Word2Vec(tokenized_sentences_roosevelt, vector_size=96, window=5, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Find words most similar to 'freedom'\n",
    "similar_to_freedom = roosevelt_embeddings.wv.most_similar('freedom')\n",
    "print(similar_to_freedom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
