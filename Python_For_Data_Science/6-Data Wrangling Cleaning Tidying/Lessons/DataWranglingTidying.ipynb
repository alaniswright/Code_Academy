{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "restaurants = pd.read_csv(\"DOHMH_restaurant_inspections.csv\")\n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))\n",
    "\n",
    "# the .shape method in pandas identifies the number of rows and columns in our dataset as (rows, columns)\n",
    "restaurants.shape  \n",
    "\n",
    "# Descriptive statistics \n",
    "restaurants.describe()\n",
    "\n",
    "# Print a concise summary of a DataFrame including all columns, null value counts and types\n",
    "restaurants.info()\n",
    "\n",
    "# the .drop_duplicates() function removes duplicate rows\n",
    "restaurants = restaurants.drop_duplicates() \n",
    "\n",
    "# map() applies the str.lower() function to each of the columns in our dataset to convert the column names to all lowercase\n",
    "restaurants.columns = map(str.lower, restaurants.columns)\n",
    "\n",
    "# axis=1` refers to the columns, `axis=0` would refer to the rows\n",
    "# In the dictionary the key refers to the original column name and the value refers to the new column name {'oldname1': 'newname1', 'oldname2': 'newname2'}\n",
    "restaurants = restaurants.rename({'dba': 'name', 'cuisine description': 'cuisine'}, axis=1)\n",
    "\n",
    "# Look at each columns datatypes\n",
    "restaurants.dtypes\n",
    "\n",
    "# .nunique() counts the number of unique values in each column \n",
    "restaurants.nunique() \n",
    "\n",
    "# counts the number of missing values in each column \n",
    "restaurants.isna().sum() \n",
    "\n",
    "\n",
    "# MISSING VALUES\n",
    "# We see that there are missing values in grade and url, but no missing values in latitude and longitude. However, we cannot have coordinates at (0.000, 0.000) for any of \n",
    "# the restaurants in our dataset, and we saw that these exist in our initial analysis. Let’s replace the (0.000,0.000) coordinates with NaN values to account for this. We \n",
    "# will use the where() function to replace the coordinates 0.000 with np.nan.\n",
    "# here our .where() function replaces latitude values less than 40 with NaN values\n",
    "restaurants['latitude'] = restaurants['latitude'].where(restaurants['latitude'] < 40, np.nan) \n",
    "\n",
    "# here our .where() function replaces longitude values greater than -70 with NaN values\n",
    "restaurants['longitude'] = restaurants['longitude'].where(restaurants['longitude'] > -70, np.nan) \n",
    "\n",
    "# .sum() counts the number of missing values in each column\n",
    "restaurants.isna().sum() \n",
    "\n",
    "# Let’s try to understand the missingness in the url column by counting the missing values across each borough. We will use the crosstab() function in pandas to do this.\n",
    "# The crosstab() computes the frequency of two or more variables. To look at the missingness in the url column we can add isna() to the column to identify if there is an \n",
    "# NaN in that column. This will return a boolean, True if there is a NaN and False if there is not. In our crosstab, we will look at all the boroughs present in our data \n",
    "# and whether or not they have missing url links.\n",
    "pd.crosstab(\n",
    " \n",
    "        # tabulates the boroughs as the index\n",
    "        restaurants['boro'],  \n",
    "\n",
    "        # tabulates the number of missing values in the url column as columns\n",
    "        restaurants['url'].isna(), \n",
    "\n",
    "        # names the rows\n",
    "        rownames = ['boro'],\n",
    "\n",
    "        # names the columns \n",
    "        colnames = ['url is na']) \n",
    "\n",
    "\n",
    "# Removing prefixes\n",
    "# .str.lstrip('https://') removes the “https://” from the left side of the string\n",
    "restaurants['url'] = restaurants['url'].str.lstrip('https://') \n",
    "\n",
    "# .str.lstrip('www.') removes the “www.” from the left side of the string\n",
    "restaurants['url'] = restaurants['url'].str.lstrip('www.') \n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))\n",
    "\n",
    "# Dealing with multiple files\n",
    "# glob can open multiple files by using regex matching to get the filenames:\n",
    "import glob\n",
    "\n",
    "files = glob.glob(\"file*.csv\")\n",
    "\n",
    "df_list = []\n",
    "for filename in files:\n",
    "  data = pd.read_csv(filename)\n",
    "  df_list.append(data)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "print(files)\n",
    "\n",
    "\n",
    "\n",
    "# Reshaping data\n",
    "\"\"\"\n",
    "we want\n",
    "\n",
    "- Each variable as a separate column\n",
    "- Each row as a separate observation\n",
    "\n",
    "We can use pd.melt() to do this transformation. .melt() takes in a DataFrame, and the columns to unpack:\n",
    "\n",
    "df = pd.melt(frame=df, id_vars=\"Account\", value_vars=[\"Checking\",\"Savings\"], value_name=\"Amount\", var_name=\"Account Type\")\n",
    "\n",
    "The parameters you provide are:\n",
    "\n",
    "frame: the DataFrame you want to melt\n",
    "id_vars: the column(s) of the old DataFrame to preserve\n",
    "value_vars: the column(s) of the old DataFrame that you want to turn into variables\n",
    "value_name: what to call the column of the new DataFrame that stores the values\n",
    "var_name: what to call the column of the new DataFrame that stores the variables\"\"\"\n",
    "\n",
    "\n",
    "# Duplicates\n",
    "duplicates = students.duplicated()\n",
    "# To check for duplicates, we can use the pandas function .duplicated(), which will return a Series telling us which rows are duplicate rows.\n",
    "\n",
    "students = students.drop_duplicates()\n",
    "#  remove all rows that are duplicates of another row.\n",
    "\n",
    "fruits = fruits.drop_duplicates(subset=['item'])\n",
    "# If we wanted to remove every row with a duplicate value in the item column, we could specify a subset:\n",
    "\n",
    "\n",
    "\n",
    "# Splitting by index\n",
    "\"\"\"Let’s say we have a column “birthday” with data formatted in MMDDYYYY format. \n",
    "In other words, “11011993” represents a birthday of November 1, 1993. \n",
    "We want to split this data into day, month, and year so that we can use these columns as separate features.\"\"\"\n",
    "# Create the 'month' column\n",
    "df['month'] = df.birthday.str[0:2]\n",
    "\n",
    "# Create the 'day' column\n",
    "df['day'] = df.birthday.str[2:4]\n",
    "\n",
    "# Create the 'year' column\n",
    "df['year'] = df.birthday.str[4:]\n",
    "\n",
    "\n",
    "# Remove old columsn from dataframe\n",
    "# create new dataframe excluding columns that are unwanted\n",
    "students = students[['full_name', 'grade', 'exam', 'score', 'age', 'gender']]\n",
    "\n",
    "\n",
    "# Splitting by character\n",
    "\"\"\"\n",
    "Let’s say we have a column called “type” with data entries in the format \"admin_US\" or \"user_Kenya\n",
    "we know that we want to split along the \"_\"\n",
    "\"\"\"\n",
    "# Split the string and save it as `string_split`\n",
    "string_split = df['type'].str.split('_')\n",
    " \n",
    "# Create the 'usertype' column\n",
    "df['usertype'] = string_split.str.get(0)\n",
    " \n",
    "# Create the 'country' column\n",
    "df['country'] = string_split.str.get(1)\n",
    "\n",
    "\n",
    "\n",
    "# To get the mean of a column in pandas, you can use the syntax:\n",
    "df.column_name.mean()\n",
    "\n",
    "# or:\n",
    "df['column_name'].mean()\n",
    "\n",
    "\n",
    "# String parsing\n",
    "# Imagine we have a price column with values like \"$1\"\n",
    "# We want to conver them to floats\n",
    "\n",
    "# First, we can use what we know of regex to get rid of all of the dollar signs:\n",
    "\n",
    "fruit.price = fruit['price'].replace('[\\$,]', '', regex=True)\n",
    "\n",
    "# Then, we can use the pandas function .to_numeric() to convert strings containing numerical values to integers or floats:\n",
    "\n",
    "fruit.price = pd.to_numeric(fruit.price)\n",
    "\n",
    "\n",
    "# EXTRACT NUMBERS FROM A STRING\n",
    "# eg. “lunges - 30 reps”\n",
    "split_df = df['exerciseDescription'].str.split('(\\d+)', expand=True)\n",
    "\n",
    "# Then, we can assign columns from this DataFrame to the original df:\n",
    "\n",
    "df.reps = pd.to_numeric(split_df[1])\n",
    "df.exercise = split_df[0].replace('[\\- ]', '', regex=True)\n",
    "\n",
    "\n",
    "# Missing values\n",
    "\n",
    "# Method 1: Drop all of the rows with a missing value\n",
    "bill_df = bill_df.dropna()\n",
    "\n",
    "# If we wanted to remove every row with a NaN value in the num_guests column only, we could specify a subset:\n",
    "bill_df = bill_df.dropna(subset=['num_guests'])\n",
    "\n",
    "\n",
    "# Method 2: fill the missing values with the mean of the column, or with some other aggregate value.\n",
    "bill_df = bill_df.fillna(value={\"bill\":bill_df.bill.mean(), \"num_guests\":bill_df.num_guests.mean()})\n",
    "\n",
    "\n",
    "\n",
    "# Get the last two digits of year column\n",
    "year\n",
    "0\t1959 AD\n",
    "\n",
    "print(df.year.str[-2:])\n",
    "\n",
    "Will give us AD"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
